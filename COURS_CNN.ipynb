{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bab66a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# CNN : Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626e564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"padding: 20px; background-color: #511f1f; border: 1px solid #e9ecef; border-radius: 5px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); max-width: 600px; margin: 30px auto;\">\n",
       "    <h3 style=\"text-align: center; color: #ffffff; margin-top: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "        Project Contributors\n",
       "    </h3>\n",
       "    <hr style=\"border-top: 1px solid #ffffff; width: 50%; margin: 10px auto;\">\n",
       "    <div style=\"text-align: center; font-size: 1.1em; color: #ffffff; margin-bottom: 15px;\">\n",
       "        <strong>Walid Canesse</strong><br>\n",
       "        <strong>Salaheddin Ben Emran</strong><br>\n",
       "        <strong>Mohamed-Ayoub Bouzid</strong>\n",
       "    </div>\n",
       "    <div style=\"text-align: center; font-size: 0.9em; color: #ffffff; font-style: italic;\">\n",
       "        All animations and visualizations were created by the authors using <a href=\"https://www.manim.community/\" target=\"_blank\" style=\"color: #aad7ff; text-decoration: underline;\">Manim</a>.\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"padding: 20px; background-color: #511f1f; border: 1px solid #e9ecef; border-radius: 5px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); max-width: 600px; margin: 30px auto;\">\n",
    "    <h3 style=\"text-align: center; color: #ffffff; margin-top: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
    "        Project Contributors\n",
    "    </h3>\n",
    "    <hr style=\"border-top: 1px solid #ffffff; width: 50%; margin: 10px auto;\">\n",
    "    <div style=\"text-align: center; font-size: 1.1em; color: #ffffff; margin-bottom: 15px;\">\n",
    "        <strong>Walid Canesse</strong><br>\n",
    "        <strong>Salaheddin Ben Emran</strong><br>\n",
    "        <strong>Mohamed-Ayoub Bouzid</strong>\n",
    "    </div>\n",
    "    <div style=\"text-align: center; font-size: 0.9em; color: #ffffff; font-style: italic;\">\n",
    "        All animations and visualizations were created by the authors using <a href=\"https://www.manim.community/\" target=\"_blank\" style=\"color: #aad7ff; text-decoration: underline;\">Manim</a>.\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64039fa",
   "metadata": {},
   "source": [
    "# Motivations\n",
    "\n",
    "The purpose of this project is to provide a clear and accessible introduction to convolutional neural networks.  \n",
    "A basic understanding of artificial neural networks is recommended.\n",
    "\n",
    "As this work has not been reviewed by professionals in the field and was produced out of personal interest alongside our studies, please feel free to contact us at **walidcanesse@gmail.com** if you notice any inaccuracies or have suggestions for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efa90b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Chapter 1 : The Intuition Behind CNNs](#Chapter-1-:-The-intuition-behind-CNNs)\n",
    "    * [I. What is an Image?](#I.-What-is-an-Image?)\n",
    "        * [A. Grayscale Images](#A.-Grayscale-Images-(Black-&-White))\n",
    "        * [B. Color Images (RGB)](#B.-Color-Images-(RGB))\n",
    "        * [C. What is a Tensor?](#C.-What-is-a-Tensor?)\n",
    "    * [II. Why Not Classic MLPs? (Example: MNIST)](#II.-Why-Not-Classic-MLPs?-(Example:-MNIST-Dataset))\n",
    "    * [III. How Humans Classify Images: Feature Detection](#III.-How-Humans-Classify-Images:-Feature-Detection)\n",
    "* [Chapter 2 : The Convolution Operator](#Chapter-2-:-The-convolution-operator)\n",
    "    * [I. Defining Image Convolution and Kernels](#I.-Defining-Image-Convolution-and-Kernels)\n",
    "    * [II. The Dimensions of a Convolution Output](#II.-The-dimensions-of-a-convolution-output)\n",
    "    * [III. Hyperparameters of Convolution](#III.-Hyperparameters-of-Convolution)\n",
    "        * [1. Padding](#1.-Padding)\n",
    "        * [2. Stride](#2.-Stride)\n",
    "        * [3. Kernel Size & Number of Filters](#3.-Kernel-Size-&-Number-of-Filters)\n",
    "    * [IV. The Final Output Dimension Formula](#IV.-The-FINAL-Output-Dimension-Formula-including-padding-and-stride-!!)\n",
    "    * [V. Feature Detection Examples](#V.-Feature-detection-examples)\n",
    "* [Chapter 3 : Building the Convolutional Neural Network](#Chapter-3-:-Building-the-Convolutional-Neural-Network)\n",
    "    * [I. From Manual Filters to Learned Features](#I.-From-Manual-Filters-to-Learned-Features)\n",
    "    * [II. How Can We See Convolution as a Neural Network Layer](#II.-How-can-we-see-Convolution-as-a-Neural-Network-Layer)\n",
    "        * [1. Definitions and Matrices](#1.-Definitions-and-Matrices)\n",
    "        * [2. Step-by-Step Calculation](#2.-Step-by-Step-Calculation-(The-Linear-Part))\n",
    "        * [3. The Activation Function](#3.-The-Activation-Function-(Non-Linearity))\n",
    "    * [III. The Convolutional Neural Network Pipeline](#III.-The-Convolutional-Neural-Network-Pipeline-!!)\n",
    "        * [1. Pooling](#1.-Pooling)\n",
    "            * [Max Pooling vs. Average Pooling](#B.-Max-Pooling-vs.-Average-Pooling)\n",
    "        * [2. CNN's Pipeline Architecture](#2.CNN's-Pipeline-!!!)\n",
    "        * [3. Designing the Pipeline: Hyperparameters](#3.-Designing-the-CNN's-Pipeline:-The-Hyperparameters)\n",
    "            * [3.4 Stabilization & Regularization (Batch Norm & Dropout)](#3.4-Crucial-Extras:-Stabilization-&-Regularization)\n",
    "        * [4. Example of How a CNN Learns (Modern Pipeline)](#4.-Example-of-how-a-CNN-learns-(The-Full-Modern-Pipeline))\n",
    "            * [4.1 Intuition vs. Reality](#4.1-Intuition-vs.-Reality)\n",
    "            * [4.2 The Forward Pass](#4.2-The-Forward-Pass-(Step-by-Step-with-Layers))\n",
    "            * [4.3 Backpropagation & Optimization](#4.3-Backpropagation-&-Optimization)\n",
    "        * [5. Network Depth vs. Information Precision](#5.-Network-Depth-vs.-Information-Precision)\n",
    "        * [6. Bonus: Mathematical Zoom into Backpropagation in CNNs](#6-bonus-mathematical-zoom-into-backpropagation-in-cnns)\n",
    "\n",
    "\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad61e9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86cc28",
   "metadata": {},
   "source": [
    "In Machine Learning, when we need to perform classification, we have many standard models that work well, such as:\n",
    "* **Logistic Regression**\n",
    "* **Random Forest**\n",
    "* **SVM (Support Vector Machines)**\n",
    "* **K-Nearest Neighbors**\n",
    "\n",
    "However, in this notebook, we will focus on a specific case: **when the input is an image**.\n",
    "\n",
    "We will demonstrate that **Neural Networks** are a much more powerful tool for this task. In particular, we will see how their structure—with a specific adaptation called **Convolution**—can be perfectly tailored to understand and classify visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e9c72",
   "metadata": {},
   "source": [
    "# Chapter 1 : The intuition behind CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0a079",
   "metadata": {},
   "source": [
    "## I. What is an Image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc4fc6",
   "metadata": {},
   "source": [
    "To a computer, an image is just a grid of numbers. However, in Deep Learning, we formalize this using the concept of **Tensors**.\n",
    "\n",
    "Dimension of an image : $Height \\times Width \\times Channels$\n",
    "\n",
    "### A. Grayscale Images (Black & White)\n",
    "We define a grayscale image as a volume of size **$N_1 \\times N_2 \\times 1$** .\n",
    "* It is a matrix where each pixel is a number between **0 and 255**.\n",
    "    * **0:** Pure Black.\n",
    "    * **255:** Pure White.\n",
    "    * **Between:** Shades of gray.\n",
    "* We can view this as a **Tensor with 1 channel** (a single \"slice\" or matrix).\n",
    "\n",
    "### B. Color Images (RGB)\n",
    "A color image is a volume of size **$N_1 \\times N_2 \\times 3$** \n",
    "* Instead of one slice, we have **3 matrices stacked together**:\n",
    "    1.  **Red** Matrix\n",
    "    2.  **Green** Matrix\n",
    "    3.  **Blue** Matrix\n",
    "* This is a **3D Tensor** with 3 channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952aaa0d",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/RealGrayscaleToMatrixV2_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/RGBAbstractTensor_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/RealGrayscaleToMatrixV2_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/RGBAbstractTensor_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc093d",
   "metadata": {},
   "source": [
    "\n",
    "### C. What is a Tensor?\n",
    "A **Tensor** is simply a generalization of matrices to higher dimensions.\n",
    "\n",
    "* **0D Tensor (Scalar):** A simple number.\n",
    "    * *Structure:* Just a singular point.\n",
    "    * *Example:* `5`\n",
    "\n",
    "* **1D Tensor (Vector):** A simple list of numbers.\n",
    "    * *Structure:* Just a line.\n",
    "    * *Example:* `[0, 255, 12, 45]`\n",
    "\n",
    "* **2D Tensor (Matrix):** A single grid of numbers.\n",
    "    * *Structure:* A \"sheet\" with rows and columns.\n",
    "    * *Example:* A single grayscale image map ($H \\times W \\times 1$).\n",
    "\n",
    "* **3D Tensor:** **Matrices stacked together**.\n",
    "    * *Structure:* A stack of sheets.\n",
    "    * *Example:* A Color Image. It is **3 matrices** (Red, Green, Blue) stacked on top of each other ($H \\times W \\times 3$).\n",
    "\n",
    "### Important Note: \n",
    "**A 3D Tensor is not limited to 3 channels.**\n",
    "\n",
    "* You can stack **2,5, 7, or even 100 matrices** together.\n",
    "* It remains a **3D Tensor** because it is still defined by 3 axes: $Height \\times Width \\times Channels$.\n",
    "* **Think of a book:** Whether a book has 3 pages or 500 pages, it is still a \"3D object\". The number of pages (channels) changes, but the structure is the same.\n",
    "\n",
    "\n",
    "Don't overcomplicate it. Just see a Tensor as a container of matrices stacked on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33634851",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
       "      <img src=\"media/videos/CNN/720p30/TensorExplanationFastSimple_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
    "      <img src=\"media/videos/CNN/720p30/TensorExplanationFastSimple_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca59ad",
   "metadata": {},
   "source": [
    "## II. Why Not Classic MLPs? (Example: MNIST Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e881d",
   "metadata": {},
   "source": [
    "Let's start with a classic example: the **MNIST dataset**.\n",
    "This is a database of handwritten digits (0 to 9). The images are **Grayscale** and have a size of **$28 \\times 28 \\times 1$**.\n",
    "\n",
    "As we discussed, grayscale images are simply matrices of pixels where values range from **0 to 255**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6c5c6",
   "metadata": {},
   "source": [
    "\n",
    "### The Naive Approach: Multi-Layer Perceptron (MLP)\n",
    "If we want to build a model to classify these digits, the first method that comes to mind is a standard **Fully Connected Neural Network** (or MLP).\n",
    "\n",
    "However, an MLP expects a **flat vector** as input, not a matrix.\n",
    "To use it, we are forced to **flatten** our $28 \\times 28$ image into a single vector of **784 pixels**. We then feed this long vector into the MLP to get a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9668f",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
       "      <img src=\"media/videos/CNN/720p30/MLP_Style_NoText_Fixed_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
    "      <img src=\"media/videos/CNN/720p30/MLP_Style_NoText_Fixed_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43880644",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "### Why is this method not optimal?\n",
    "While this might work for very simple tasks, it has **three major problems**:\n",
    "\n",
    "#### 1. Loss of Spatial Information\n",
    "An image is not just a random list of numbers; it is a **2D structured object**.\n",
    "* Neighboring pixels are correlated (they work together to form shapes).\n",
    "* The order and position matter.\n",
    "**Problem:** By flattening the image into a vector, we **break this structure**. The network reads the image \"pixel by pixel\" without understanding the geometry, leading to a poor use of visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767002f4",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
       "      <img src=\"media/videos/CNN/720p30/FlatteningSimple_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
    "      <img src=\"media/videos/CNN/720p30/FlatteningSimple_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055a26a",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "#### 2. Giant Number of Parameters\n",
    "Let's do the math for a simple MLP with **2 hidden layers of 100 neurons each** and **10 output neurons**.\n",
    "\n",
    "* **Input Size:** 784 (pixels)\n",
    "* **Hidden Layer 1:** 100 neurons\n",
    "* **Hidden Layer 2:** 100 neurons\n",
    "* **Output Layer:** 10 neurons\n",
    "\n",
    "**Detailed Calculation of Parameters (Weights + Biases):**\n",
    "\n",
    "* **Layer 1 (Input $\\to$ Hidden 1):**\n",
    "    * Weights: $784 \\times 100 = 78,400$\n",
    "    * Biases: $100$ (1 per neuron)\n",
    "    \n",
    "* **Layer 2 (Hidden 1 $\\to$ Hidden 2):**\n",
    "    * Weights: $100 \\times 100 = 10,000$\n",
    "    * Biases: $100$\n",
    "\n",
    "* **Layer 3 (Hidden 2 $\\to$ Output):**\n",
    "    * Weights: $100 \\times 10 = 1,000$\n",
    "    * Biases: $10$\n",
    "\n",
    "**Total Sum:**\n",
    "$$78,400 + 100 + 10,000 + 100 + 1,000 + 10 = \\mathbf{89,610}$$\n",
    "\n",
    "**Problem:** We have nearly **90,000 parameters** just for a tiny $28 \\times 28$ black and white image. On realistic color images (e.g., $1000 \\times 1000$), this number would explode into the billions, making the model impossible to train efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db6b29",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "#### 3. No Translation Invariance\n",
    "To a human, if a digit \"5\" moves a little bit to the left or right, it is clearly still a \"5\". We recognize the **shape**, no matter where it is.\n",
    "**Problem:** An MLP looks at each specific pixel position separately.\n",
    "* If the \"5\" shifts, different input pixels light up.\n",
    "* To the MLP, this looks like a completely different input.\n",
    "* It has to \"re-learn\" what a \"5\" looks like for every possible position in the image. It lacks **Translation Invariance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fe704",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
       "      <img src=\"media/videos/CNN/720p30/DigitShiftBig_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
    "      <img src=\"media/videos/CNN/720p30/DigitShiftBig_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c04a1",
   "metadata": {},
   "source": [
    "## III. How Humans Classify Images: Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0780a",
   "metadata": {},
   "source": [
    "To understand the intuition behind CNNs, let's look at how humans see.\n",
    "\n",
    "When we look at an image, **we do not scan every single pixel one by one.**\n",
    "Instead, we process the image globally. We unconsciously look for **Features**:\n",
    "* **Edges** (contours)\n",
    "* **Textures**\n",
    "* **Patterns** (e.g., the shape of an ear, the curve of a digit).\n",
    "\n",
    "**The Decision Process:**\n",
    "Identifying these features increases the probability of a specific class. If we see \"whiskers\" + \"pointed ears\" + \"fur\", our brain concludes: *\"Yes, this looks like a cat.\"*\n",
    "\n",
    "**The Goal of CNNs:**\n",
    "Convolutional Neural Networks are designed to **mimic this exact behavior**.\n",
    "The key is the convolution operator who acts like an **Automatic Feature Detector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a93b4da",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"assets/STRATEGIE1.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"assets/STRATEGIE2.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"assets/STRATEGIE1.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"assets/STRATEGIE2.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582d743",
   "metadata": {},
   "source": [
    "# Chapter 2 : The convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2007a",
   "metadata": {},
   "source": [
    "## I. Defining Image Convolution and Kernels\n",
    "\n",
    "As we said, the convolution acts as a **Feature Detector**. But how exactly do we compute it?\n",
    "\n",
    "**The Process:**\n",
    "1.  **Input:** We take an image (pixel matrix).\n",
    "2.  **Kernel:** We define a filter of a specific size that we choose.\n",
    "3.  **Operation:** We slide the kernel over the image, performing a \"dot product\" (multiplication + sum) at every position.\n",
    "\n",
    "Let's do examples !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34f2a1",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "### 1. 1 CHANNEL INPUT WITH 1 FILTER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4cff6",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/Architecture_1_Grey_1Filter_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/ConvExampleExplainedClean_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/Architecture_1_Grey_1Filter_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/ConvExampleExplainedClean_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d9040",
   "metadata": {},
   "source": [
    "### 2. 1 CHANNEL INPUT WITH 2 FILTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b2b6",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/Architecture_2_Grey_2Filters_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/Conv2FiltersExample_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/Architecture_2_Grey_2Filters_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/Conv2FiltersExample_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802eb09",
   "metadata": {},
   "source": [
    "### 3. 3 CHANNEL INPUT WITH 1 FILTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a98ae",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/Architecture_3_RGB_1Filter_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/RGBConvStepByStepCentered_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/Architecture_3_RGB_1Filter_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/RGBConvStepByStepCentered_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96d721",
   "metadata": {},
   "source": [
    "### 4. 3 CHANNEL INPUT WITH 5 FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bd3f2",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
       "      <img src=\"media/videos/CNN/720p30/FullCNNSequence_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    <td style=\"width: 100%; border: none; padding: 5px; text-align: center;\">\n",
    "      <img src=\"media/videos/CNN/720p30/FullCNNSequence_ManimCE_v0.19.0.gif\" style=\"width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fd415",
   "metadata": {},
   "source": [
    "## II. The dimensions of a convolution output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91ac6e",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "(N \\times N \\times C) \\otimes (F \\times F \\times C \\times M) \\rightarrow (N - F + 1) \\times (N - F + 1) \\times M\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* **N**: Size of the input\n",
    "* **F**: Size of the Kernel\n",
    "* **C**: Channels\n",
    "* **M**: Number of filters\n",
    "\n",
    "> **⚠️ Warning:**\n",
    "> The input and the kernels **must have the same number of channels** ($C$).\n",
    "> You cannot convolve a $3 \\times 3 \\times 1$ input with a $2 \\times 2 \\times 3$ kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb31cd0",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/ConvDimensionsSimpleCentered_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/ConvDimensionsSimpleCentered_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09659ec7",
   "metadata": {},
   "source": [
    "## III. Hyperparameters of Convolution\n",
    "\n",
    "When designing a CNN, we don't just \"apply convolution.\" We have to tune specific knobs—called **Hyperparameters**—to control how the network processes the image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455f945",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "### 1. Padding\n",
    "**The Problem:** Without padding, the pixels on the borders are \"seen\" less often by the filters because we cannot center the kernel on them. (Try a simple example yourself: you will notice that edge pixels are involved in far fewer calculations than the central pixels).\n",
    "\n",
    "**The Solution:** Padding consists of adding a border of pixels (usually **Zeros**) around the input image.It allows us to process the edges almost as effectively as the center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044519ae",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/PaddingOneByOneEng_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/PaddingOneByOneEng_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f028f8",
   "metadata": {},
   "source": [
    "### 2. Stride\n",
    "**The Concept:** The Stride is the \"step size\" of the convolution.\n",
    "* **Stride = 1:** We shift the filter **1 pixel** at a time. This is the standard detailed scan.\n",
    "* **Stride = 2:** We shift the filter **2 pixels** at a time (we skip one pixel).\n",
    "**Impact:** A larger stride **reduces the output size**  and speeds up computation because we perform fewer operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705a00d",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/Stride4x4Comparison_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/Stride4x4Comparison_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26b6ba",
   "metadata": {},
   "source": [
    "### 3. Kernel Size & Number of Filters\n",
    "Finally, we must choose the properties of the filters themselves:\n",
    "\n",
    "* **Kernel Size (e.g., $3 \\times 3$ or $5 \\times 5$):**\n",
    "    * This controls the **Receptive Field** (the local area) the network looks at.\n",
    "    * Small kernels look for fine details. Large kernels look for broader patterns.\n",
    "\n",
    "* **Number of Filters (Depth):**\n",
    "    * This controls **how many features** we want to learn in this layer.\n",
    "    * **More filters** = The network can learn more diverse patterns (edges, textures, colors).\n",
    "    * **Trade-off:** More filters mean more parameters to learn and slower training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fa472",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/KernelSizeAndDepthClean_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/KernelSizeAndDepthClean_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbb517",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "## IV. The FINAL Output Dimension Formula including padding and stride !!\n",
    "\n",
    "* **$N$**: Input dimension (Height = Width).\n",
    "* **$F$**: Filter dimension (Height = Width).\n",
    "* **$P$**: Padding.\n",
    "* **$S$**: Stride.\n",
    "\n",
    "The formula to calculate the output size of the feature map is:\n",
    "\n",
    "$$\n",
    "\\text{Output Size} = \\left\\lfloor \\frac{N - F + 2P}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "> **Example:**\n",
    "> * Input Image ($N$): $28$ (for a $28 \\times 28$ image)\n",
    "> * Filter ($F$): $3$ (for a $3 \\times 3$ kernel)\n",
    "> * Padding ($P$): $1$\n",
    "> * Stride ($S$): $1$\n",
    ">\n",
    "> $$\\text{Size} = \\frac{28 - 3 + (2 \\times 1)}{1} + 1 = \\frac{27}{1} + 1 = \\mathbf{28}$$\n",
    "> *Result:* We kept the same size ($28 \\times 28$) thanks to the padding!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7224f8c",
   "metadata": {},
   "source": [
    "# V. Feature detection examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a5584",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/FeatureDetectionGray_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/FeatureDetectionRGB_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/FeatureDetectionGray_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/FeatureDetectionRGB_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c04abd",
   "metadata": {},
   "source": [
    "# Chapter 3 : Building the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7021ab9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae794e6",
   "metadata": {},
   "source": [
    "Now that we understand the convolution operator, we can move on to how it helps us build **image classification models**.\n",
    "\n",
    "## The guiding principle for CNNs is as follows: \n",
    "\n",
    "1. **Convolution part** : We take our input image and perform a convolution with a set of filters. Then, we perform convolutions again on these outputs with new filters, and repeat the process... (the number of times we repeat this is a hyperparameter that we choose ourselves).\n",
    "\n",
    "2. **MLP part** : Once we have performed as many compositions of convolutions as desired, we take the final outputs, apply a flattening operation, and feed the result into an MLP to perform the classification. \n",
    "\n",
    "This is the core concept to keep in mind.\n",
    "\n",
    "\n",
    "## Wait, didn't we say flattening was bad?\n",
    "\n",
    "You might be looking at this architecture and thinking:\n",
    "> *\"In the beginning, we said flattening the image destroys spatial structure, requires too many parameters, and lacks translation invariance. Yet, here we are flattening the output at the end. Is this a contradiction?\"*\n",
    "\n",
    "The short answer is **no**. The key difference is **WHEN** we flatten.\n",
    "In the naive approach, we flattened **raw pixels**. Here, we flatten **extracted features**.\n",
    "\n",
    "Let's revisit the three major problems to see how this architecture solves them:\n",
    "\n",
    "#### 1. Loss of Spatial Information\n",
    "* **The Old Problem:** Flattening the raw image immediately destroyed the 2D grid. The model lost the concept of \"neighbors\" (pixel $0,0$ and $0,1$) before it could even process shapes.\n",
    "* **The Solution:** Here, we do **not** flatten immediately. We perform the convolution steps **first**. These steps respect and utilize the 2D structure to detect edges, curves, and patterns. By the time we flatten at the very end, we are no longer flattening \"pixels\"; we are flattening a list of **high-level concepts** (e.g., \"contains a circle,\" \"has vertical lines\"). The spatial information has already been processed and encoded.\n",
    "\n",
    "#### 2. Giant Number of Parameters\n",
    "* **The Old Problem:** Connecting every single pixel of a large image (e.g., $1000 \\times 1000$) to a hidden layer created millions of wasteful connections.\n",
    "* **The Solution:** As we apply successive convolutions, the spatial dimensions of the image ($Height \\times Width$) typically **shrink**. instead of flattening a massive $1000 \\times 1000$ image, we might end up flattening a small $7 \\times 7$ feature map.\n",
    "    * Even with multiple filters, the total input size entering the MLP is significantly smaller.\n",
    "    * This drastic reduction in size means far fewer parameters are needed in the final layers.\n",
    "\n",
    "#### 3. No Translation Invariance\n",
    "* **The Old Problem:** In a standard MLP, if a shape moved from the left to the right, it was seen as a completely new input.\n",
    "* **The Solution:** The convolution operation handles the location. Because we slide the **same filter** across the whole image, a specific feature (like a curve) will be detected regardless of where it is.\n",
    "    * The convolutional layers do the hard work of *locating* the features.\n",
    "    * The final MLP doesn't need to search for the object; it just receives a signal from the feature maps saying *\"I found this pattern\"* (regardless of where it was originally located)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3d0fe",
   "metadata": {},
   "source": [
    "# I. From Manual Filters to Learned Features\n",
    "\n",
    "### The Hypothesis\n",
    "We might ask:\n",
    "> *\"Why don't we just manually choose the best kernels (like standard edge detectors), apply them to the image to extract features, and then feed the result into a classic MLP for classification?\"*\n",
    "\n",
    "### The Problem\n",
    "This approach is flawed for three main reasons:\n",
    "1.  **Missing Hidden Features:** We don't always know intuitively which patterns are the most \"discriminant\" (useful) to distinguish between classes.\n",
    "2.  **Unknown Combinations:** We don't know the optimal mix of kernels to use.\n",
    "3.  **Human Limitation:** By choosing manually, we limit the network to features *we* have imagined.\n",
    "\n",
    "### The Solution: End-to-End Learning\n",
    "The core idea of CNNs is to **let the network learn the values inside the kernels itself.**\n",
    "\n",
    "* **Learnable Parameters:** The coefficients inside the filters are not fixed constants. They are **parameters** (weights) of the network.\n",
    "* **Backpropagation:** Just like in an MLP, these weights are updated during training. The network figures out *on its own* which kernels are best to minimize the error.\n",
    "\n",
    "### The Result: A Giant Pipeline / Neural Network\n",
    "We obtain a single, unified neural network where:\n",
    "1.  **Convolution Layers (The \"Eyes\"):** Learn to be the best possible **Feature Extractors**. This part of the neural network **will not be fully connected** ( we will see that after ).\n",
    "\n",
    "\n",
    "2.  **Fully Connected Layers (The \"Brain\"):** Take these features and handle the **Classification Decision**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c189e6c",
   "metadata": {},
   "source": [
    "# II. How can we see Convolution as a Neural Network Layer\n",
    "\n",
    "To fully understand why a Convolutional Layer can be considered a specific type of Neural Network layer, we must examine the mathematical operations in detail. We will deconstruct a minimal example to draw the parallel with standard dense layers.\n",
    "\n",
    "Let us consider a simplified case:\n",
    "* **Input ($X$):** A $3 \\times 3$ image (or feature map).\n",
    "* **Filter ($W$):** A $2 \\times 2$ kernel.\n",
    "* **Bias ($b$):** A scalar value associated with this specific filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622954e6",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Definitions and Matrices\n",
    "\n",
    "First, let us explicitly define our matrices.\n",
    "\n",
    "**The Input Matrix ($x$):**\n",
    "This represents the pixel values of our image.\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**The Kernel Matrix ($w$):**\n",
    "These are the **weights** of the network that will be learned via backpropagation.\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix} \n",
    "w_1 & w_2 \\\\\n",
    "w_3 & w_4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**The Bias ($b$):**\n",
    "⚠️ **Crucial Point:** Just like a standard neuron ($y = wx + b$), a convolution filter will always includes a **Bias**.\n",
    "* Even though the output is a $2 \\times 2$ matrix ($4$ weights), there is only **ONE single bias value** ($b$) shared across the entire feature map.\n",
    "\n",
    "**The Output Matrix ($z$):**\n",
    "Based on the dimensions, a $2 \\times 2$ kernel sliding over a $3 \\times 3$ input produces a $2 \\times 2$ output.\n",
    "$$\n",
    "Z = \n",
    "\\begin{bmatrix} \n",
    "z_{1} & z_{2} \\\\\n",
    "z_{3} & z_{4} \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15869e62",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Step-by-Step Calculation (The Linear Part)\n",
    "\n",
    "Let us compute the values of the output matrix $Z$. The operation consists of a dot product, to which we add the shared bias.\n",
    "\n",
    "#### Step A: Calculating $z_1$ (Top-Left)\n",
    "We position the kernel over the top-left region of the input (covering $x_{11}, x_{12}, x_{21}, x_{22}$).\n",
    "\n",
    "The calculation is:\n",
    "$$z_1 = (x_{11} \\cdot w_1) + (x_{12} \\cdot w_2) + (x_{21} \\cdot w_3) + (x_{22} \\cdot w_4) + \\mathbf{b}$$\n",
    "\n",
    "**Neural Network Interpretation:**\n",
    "Observe this equation. It is the standard formula for a neuron: $\\sum (x_i \\cdot w_i) + b$.\n",
    "* The neuron $z_1$ is connected to **only 4 specific inputs** ($x_{11}, x_{12}, x_{21}, x_{22}$).\n",
    "* In a standard Fully Connected layer, $z_1$ would be connected to all 9 inputs.\n",
    "* This property is called **Sparse Connectivity** (look at the animation at the very end of II. to convince yourself).\n",
    "\n",
    "#### Step B: Calculating $z_2$ (Top-Right)\n",
    "We slide the kernel one pixel to the right (Stride = 1). It now covers $x_{12}, x_{13}, x_{22}, x_{23}$.\n",
    "\n",
    "The calculation is:\n",
    "$$z_2 = (x_{12} \\cdot \\mathbf{w_1}) + (x_{13} \\cdot \\mathbf{w_2}) + (x_{22} \\cdot \\mathbf{w_3}) + (x_{23} \\cdot \\mathbf{w_4}) + \\mathbf{b}$$\n",
    "\n",
    "**Neural Network Interpretation:**\n",
    "Notice that we use **exactly the same weights** ($w_1, w_2, w_3, w_4$) and the **same bias** ($b$) as we did for $z_1$.\n",
    "* The neuron $z_2$ looks at a different local region.\n",
    "* However, it shares the same parameters as $z_1$.\n",
    "* This property is called **Parameter Sharing**.\n",
    "\n",
    "#### Step C: Completing the Matrix\n",
    "We continue sliding the window to compute the remaining outputs:\n",
    "\n",
    "* **Bottom-Left ($z_3$):**\n",
    "    $$z_3 = (x_{21} \\cdot w_1) + (x_{22} \\cdot w_2) + (x_{31} \\cdot w_3) + (x_{32} \\cdot w_4) + \\mathbf{b}$$\n",
    "\n",
    "* **Bottom-Right ($z_4$):**\n",
    "    $$z_4 = (x_{22} \\cdot w_1) + (x_{23} \\cdot w_2) + (x_{32} \\cdot w_3) + (x_{33} \\cdot w_4) + \\mathbf{b}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7d611",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/ConvView_Final_Title_SharedBias_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/ConvView_Final_Title_SharedBias_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b21810",
   "metadata": {},
   "source": [
    "\n",
    "### 3. The Activation Function (Non-Linearity)\n",
    "\n",
    "The calculations above yield a linear output map $Z$. To create a neural network capable of learning complex patterns, we must introduce non-linearity.\n",
    "\n",
    "Just like in a standard MLP, we apply an **Activation Function** (typically ReLU) element-wise to the matrix $Z$.\n",
    "\n",
    "Let $A$ be the final Activation Map:\n",
    "$$\n",
    "A = \\text{ReLU}(Z) = \n",
    "\\begin{bmatrix} \n",
    "\\max(0, z_1) & \\max(0, z_2) \\\\\n",
    "\\max(0, z_3) & \\max(0, z_4) \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652da07",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/ActivationFunctionStep_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/ActivationFunctionStep_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d454c",
   "metadata": {},
   "source": [
    "### 4. Summary: Convolution vs. Fully Connected\n",
    "\n",
    "We have demonstrated that a Convolution is simply a Neural Network layer with two specific structural constraints:\n",
    "\n",
    "1.  **Local/Sparse Connectivity:** Each output neuron is connected only to a small, local subset of the input pixels. It does not \"see\" the whole image at once.\n",
    "2.  **Weight & Bias/Parameters Sharing:** All neurons in a specific feature map share the exact same weights (filters) and the same bias. This forces the network to search for the same feature everywhere in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8255ec36",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"assets/output.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"assets/output.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d9f1f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have demonstrated that a **Convolution** combined with a **Bias** and an **Activation** ($\\sigma$) is functionally identical to a standard neural network layer: $$y = \\sigma(W * x + b)$$\n",
    "\n",
    "---\n",
    "\n",
    "> <span style=\"color:red\">**⚠️ CRITICAL WARNING: Bias is per Filter**</span>\n",
    ">\n",
    "> Bias is a single scalar value **per filter**. It is NOT per pixel and NOT per input channel. This single value is **broadcasted** (added) to the entire 2D output map produced by that filter.\n",
    "\n",
    "---\n",
    "\n",
    "### Dimension Examples\n",
    "\n",
    "We use the format: $F \\times F \\times Channels_{In} \\times N_{Filters}$ (where F is filter size).\n",
    "\n",
    "#### Example 1: 1-Channel Input (Grayscale)\n",
    "*Input:* $28 \\times 28 \\times \\mathbf{1}$ image.\n",
    "*Layer:* 32 filters of size $3 \\times 3$.\n",
    "\n",
    "* **Kernel Shape:** $3 \\times 3 \\times \\mathbf{1} \\times 32$\n",
    "* **Bias:** 32 parameters (1 per output map).\n",
    "\n",
    "[Image showing a 1-channel convolution process: a single 2D input matrix convolved with a 2D kernel, producing one 2D output map to which one bias value is added.]\n",
    "\n",
    "#### Example 2: 3-Channel Input (RGB)\n",
    "*Input:* $32 \\times 32 \\times \\mathbf{3}$ image.\n",
    "*Layer:* 64 filters of size $5 \\times 5$.\n",
    "\n",
    "* **Kernel Shape:** $5 \\times 5 \\times \\mathbf{3} \\times 64$\n",
    "* **Bias:** 64 parameters.\n",
    "* *Note:* The kernel depth (3) MUST match the input depth. These 3 channels are summed into a single 2D map *before* the single bias is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ef524",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/DimensionExamplesSideBySide_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/DimensionExamplesSideBySide_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8ceee",
   "metadata": {},
   "source": [
    "# III. The Convolutional Neural Network Pipeline !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee5a41",
   "metadata": {},
   "source": [
    "## 1. Pooling\n",
    "\n",
    "The pipeline will include a step called **Pooling** that we often add after a convolution.\n",
    "\n",
    "We present this step before going straight to the full CNN pipeline.\n",
    "\n",
    "Its goal is to reduce the spatial dimensions of the image (Height and Width), which drastically reduces the number of parameters and computation cost, while preserving most important informations.\n",
    "\n",
    "### A. How it Works\n",
    "We define a window of size **$K \\times K$** (just like a kernel) and a **Stride**.\n",
    "\n",
    "* **General Rule:** The window slides over the image. For each window, we compress the pixels into a single value (we calculate either the **max** or the **average** of those values).\n",
    "* **Standard Practice:** In 99% of cases, we use a window of **$2 \\times 2$** with a **Stride of 2**. This effectively divides the height and width of the image by 2 at each step.\n",
    "\n",
    "\n",
    "### B. Max Pooling vs. Average Pooling\n",
    "There are two main ways to compress the data:\n",
    "1.  **Average Pooling:** Calculates the average value of the pixels in the window.\n",
    "2.  **Max Pooling:** Selects only the **maximum value** in the window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b765793",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"media/videos/CNN/720p30/TinyMaxPoolingFixed_ManimCE_v0.19.0.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/videos/CNN/720p30/TinyMaxPoolingFixed_ManimCE_v0.19.0.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b486810",
   "metadata": {},
   "source": [
    "\n",
    "### C. Why is Max Pooling Better for Classification?\n",
    "Let's take a concrete example with a $2 \\times 2$ window to understand why Max Pooling is the standard.\n",
    "Imagine a region of the image after a convolution (a feature map):\n",
    "\n",
    "$$\n",
    "\\text{Window} = \\begin{bmatrix} \\mathbf{164} & 0 \\\\ 1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Context:**\n",
    "    * **164:** A very strong activation (the filter detected a sharp feature, like an edge or a texture).\n",
    "    * **0 and 1:** Background noise. Note that in image processing, dark pixels (values near 0) usually do not contain key information; they represent empty space or background.\n",
    "\n",
    "**The Comparison:**\n",
    "\n",
    "* **Average Pooling:**\n",
    "    $$\\frac{164 + 0 + 1 + 0}{4} \\approx \\mathbf{41}$$\n",
    "    * *Result:* The strong signal (164) is **diluted** by the zeros. The information becomes blurry and less distinct.\n",
    "\n",
    "* **Max Pooling:**\n",
    "    $$\\max(164, 0, 1, 0) = \\mathbf{164}$$\n",
    "    * *Result:* We keep the **164**. The pooling ignores the noise (the zeros) and preserves the most important feature.\n",
    "\n",
    "**Conclusion:** For image classification, Max Pooling acts as a \"Feature Selector,\" ensuring the strongest patterns survive, while Average Pooling tends to wash them out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e605f",
   "metadata": {},
   "source": [
    "### Pooling as a Layer of a Neural Network\n",
    "\n",
    "We can consider the Pooling step as a genuine layer within the Neural Network just like convolution did !\n",
    "\n",
    "* **Activation Function:** It applies a specific non-linear function over its inputs:\n",
    "  $$\\sigma(x_1, ..., x_n) = \\max(x_1, ..., x_n)$$\n",
    "\n",
    "* **No Weights, No Bias:** Unlike Convolutional or Dense layers, a Pooling layer has **no weights ($W$) and no bias ($b$)**.\n",
    "  * *Number of Trainable Parameters = 0.*\n",
    "\n",
    "* **Role:** Consequently, this layer does not \"learn\" features itself. It strictly applies a fixed mathematical operation to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d2517",
   "metadata": {},
   "source": [
    "## 2.CNN's Pipeline !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32b3a5",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border: none; background: transparent;\">\n",
       "  <tr style=\"background: transparent;\">\n",
       "    \n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/TinyCNN_ImageInput_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
       "      <img src=\"media/videos/CNN/720p30/NeuralNetworkAnimation_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
       "    </td>\n",
       "\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<table style=\"width: 100%; border: none; background: transparent;\">\n",
    "  <tr style=\"background: transparent;\">\n",
    "    \n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/TinyCNN_ImageInput_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "    <td style=\"width: 50%; border: none; padding: 5px;\">\n",
    "      <img src=\"media/videos/CNN/720p30/NeuralNetworkAnimation_ManimCE_v0.19.0.gif\" style=\"width: 100%;\">\n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f205a7",
   "metadata": {},
   "source": [
    "\n",
    "We can view a CNN in two ways: functionally as a **pipeline of blocks**, or structurally as **one giant Neural Network** (look upward).\n",
    "\n",
    "### A. The Pipeline (Standard Architecture)\n",
    "Most classical CNNs (like VGG or LeNet) follow this specific recipe:\n",
    "\n",
    "$$\n",
    "\\text{Input} \\xrightarrow{} \\underbrace{[ \\text{Conv} \\xrightarrow{\\text{ReLU}} \\text{Pool} ] \\times N}_{\\text{Feature Extraction}} \\xrightarrow{\\text{Flatten}} \\underbrace{\\text{Dense} \\xrightarrow{\\text{ReLU}} \\dots \\xrightarrow{\\text{Softmax}}}_{\\text{Classification (MLP)}}\n",
    "$$\n",
    "\n",
    "> **⚠️ Critical Insight: It is ONE Network**\n",
    "> Do not be fooled by the blocks. The Convolutional part **IS** a neural network, but with **sparse connections** (local receptive fields) and **shared weights** (kernels). The whole system is trained end-to-end via Backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Key Design Choices\n",
    "\n",
    "#### 1. Activation Function: Why ReLU?\n",
    "We use **ReLU** ($f(x) = \\max(0, x)$) almost everywhere instead of Sigmoid/Tanh.\n",
    "* **No Vanishing Gradient:** Gradients remain strong (1) for positive inputs, speeding up training.\n",
    "* **Efficiency:** Computationally essentially free.\n",
    "\n",
    "#### 2. Choosing Dimensions (Heuristics)\n",
    "* **Spatial Size ($\\downarrow$):** We **reduce** height/width at each step (via Pooling) to compress information.\n",
    "* **Depth / Filters ($\\uparrow$):** We **increase** the number of filters deeper in the network (e.g., $32 \\to 64 \\to 128$).\n",
    "    * *Reason:* Early layers detect simple edges (few filters needed). Deep layers detect complex combinations (many filters needed).\n",
    "* **Kernel Size:** **$3 \\times 3$** is the industry standard. It is the smallest symmetrical filter that captures the notion of \"center\" and \"neighbors\" efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87e8b4",
   "metadata": {},
   "source": [
    "## 3. Designing the CNN's Pipeline: The Hyperparameters\n",
    "\n",
    "We can divide them into three main categories:\n",
    "\n",
    "### 3.1 Convolution Layers\n",
    "\n",
    "**Step 1: Define Architecture Depth**\n",
    "* **Number of Blocks:** How many (Conv + Pooling) blocks to stack?\n",
    "\n",
    "**Step 2: For EACH Block, choose:**\n",
    "* **Number of Filters:** (e.g., 32, 64, 128...)\n",
    "* **Filter Size (Kernel):** (e.g., $3 \\times 3$)\n",
    "* **Padding:** \n",
    "* **Stride:** \n",
    "* **Activation Function:** (e.g., ReLU,sigmoid)\n",
    "* **Pooling Type:** (Max Pooling vs Average Pooling)\n",
    "\n",
    "### 3.2 MLP's Design (Classifier)\n",
    "* **Number of Layers**\n",
    "* **Neurons per Layer**\n",
    "* **Activation Function:** \n",
    "\n",
    "### 3.3 Training (Backward)\n",
    "* **Optimizer:** (e.g., Adam, SGD)\n",
    "* **Learning Rate**\n",
    "* **Batch Size**\n",
    "* **Number of Epochs**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e48a5f",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Crucial Extras: Stabilization & Regularization\n",
    "To make the model robust, we add two specific layers.\n",
    "\n",
    "#### 3.4.1 Batch Normalization (Batch Norm)\n",
    "* **Where to put it?**\n",
    "    * **In the Conv Block:** Place it **between** the Convolution and the Activation.\n",
    "        * *Order:* `Conv2d` $\\to$ `BatchNorm2d` $\\to$ `ReLU`.\n",
    "    * **In the MLP Block:** Place it **between** the Linear layer and the Activation for all **hidden layers**.\n",
    "        * *Order:* `Linear` $\\to$ `BatchNorm1d` $\\to$ `ReLU`.\n",
    "\n",
    "* **⚠️ Crucial Exception:**\n",
    "    * **The Output Layer:** Do **NOT** batch normalize the final layer before the Softmax.\n",
    "    * *Reason:* We need the raw scores (logits, $xW+b$) to preserve the relative scale between classes so Softmax can compute correct probabilities.\n",
    "\n",
    "* **The Math:** It normalizes the **raw output** $z = xW + b$ (coming out of the Convolution or Linear layer) using the **mean ($\\mu$) and variance ($\\sigma^2$) of the current mini-batch**.\n",
    "    $$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
    "    * *$\\mu$ and $\\sigma$:* Calculated strictly on the **current batch** (not the whole dataset).\n",
    "    * *$\\gamma$ and $\\beta$:* Learnable parameters that allow the network to adjust the scale and shift.\n",
    "    * *$\\epsilon$:* A tiny number added to ensure we **never divide by 0**.\n",
    "* **Why?** It stabilizes the learning process (keeps values centered), allowing the use of higher learning rates and faster convergence.\n",
    "\n",
    "If you want to know more about Batch Normalization , have a look here : \n",
    "https://www.youtube.com/watch?v=nT9nKBCjS_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3b8df",
   "metadata": {},
   "source": [
    "#### 3.4.2 Dropout\n",
    "\n",
    "**Dropout** is also a regularization technique.\n",
    "We will want to randomly \"turn off\" some neurons in our Neural Network in order to make better predictions (yes it does seems a little counterintuitive at first).\n",
    "\n",
    "**1. Biological Intuition (The Origin Story)**\n",
    "\n",
    "The concept of Dropout is directly inspired by how the human brain functions:\n",
    "* **Energy Efficiency:** The brain consumes a significant amount of energy. Biologically, it is not sustainable for all neurons to be turned on simultaneously for every stimulus.\n",
    "* **Sparsity (Specialization):** The brain prefers **sparse representations**. To recognize a \"cat,\" only a small fraction of highly specialized neurons need to activate.\n",
    "* **Robustness & The Parsimony Principle:**\n",
    "    * *Biological Fact:* Brain neurons naturally die or misfire, yet our cognitive abilities remain stable. We don't lose the ability to recognize a cat just because one neuron failed.\n",
    "    * *Occam's Razor:* This forces the brain to adhere to the **Principle of Parsimony (Occam's Razor)**: \"The simplest explanation is usually the best.\"\n",
    "    * *The Bridge to Overfitting:* By applying Dropout, we mimic this biological constraint. We force the network to make accurate predictions using only a random fraction of its neurons at any given time.\n",
    "\n",
    "**2. The Context: Fighting Overfitting**\n",
    "\n",
    "A fundamental number in Deep Learning is the ratio between the number of parameters and the amount of data.\n",
    "* **The Critical Case:** $N_{parameters} \\gg N_{data}$.\n",
    "* **The Risk:**  Overfitting.\n",
    "\n",
    "To fight overfitting, we often use **Data Augmentation** (flipping images, zooming, etc.) to artificially increase the dataset size.\n",
    "Dropout is essentially a form of Data Augmentation, by randomly turning off some neurons you corrupt the data going through the Neural Network.This forces the model to process a slightly different (noisy) representation of the input at each iteration.\n",
    "\n",
    "\n",
    "**3. Mathematics and Algorithm (Step-by-Step)**\n",
    "\n",
    "Let's formalize the process for a hidden layer $l$.\n",
    "* Let $z^{(l)}$ be the activation vector (before Dropout) of dimension $N$.\n",
    "* Let $p$ be the **Dropout Rate** (the probability of zeroing out a neuron, e.g., $p=0.5$).\n",
    "\n",
    "**The Operation: Inverted Dropout**\n",
    "Modern frameworks (PyTorch/TensorFlow) use **Inverted Dropout** to simplify the testing phase.\n",
    "\n",
    "1.  **Generate Bernoulli Mask:**\n",
    "    We generate a vector $m^{(l)}$ of the same size as $z^{(l)}$, composed of 0s and 1s.\n",
    "    $$m_i^{(l)} \\sim \\text{Bernoulli}(1-p)$$\n",
    "    *(Probability of being 1 is $1-p$, probability of being 0 is $p$).*\n",
    "\n",
    "2.  **Apply Mask:**\n",
    "    $$\\tilde{z}^{(l)} = z^{(l)} \\odot m^{(l)}$$\n",
    "    *($\\odot$ denotes the element-wise / Hadamard product).*\n",
    "\n",
    "3.  **Scaling:**\n",
    "    To maintain the same expected \"energy\" (mathematical expectation) as if all neurons were active, we divide by the keep probability $(1-p)$.\n",
    "    $$\\text{Output}_{train} = \\frac{\\tilde{z}^{(l)}}{1-p}$$\n",
    "\n",
    "**Detailed Numerical Example**\n",
    "Imagine a layer with **5 neurons**.\n",
    "* Raw Output ($z$): `[10, 20, 30, 40, 50]`\n",
    "* Dropout Rate ($p$): **0.4** (We kill 40% of neurons).\n",
    "* Scaling Factor: $\\frac{1}{1-0.4} = \\frac{1}{0.6} \\approx 1.67$\n",
    "\n",
    "* **Iteration 1 (Batch A):** Randomness kills neurons **2** and **4**.\n",
    "    * Mask: `[1, 0, 1, 0, 1]`\n",
    "    * Apply: `[10, 0, 30, 0, 50]`\n",
    "    * Scale ($\\times 1.67$): `[16.7, 0, 50.1, 0, 83.5]`\n",
    "\n",
    "* **Iteration 2 (Batch B):** Randomness kills neurons **1** and **5**.\n",
    "    * Mask: `[0, 1, 1, 1, 0]`\n",
    "    * Apply: `[0, 20, 30, 40, 0]`\n",
    "    * Scale ($\\times 1.67$): `[0, 33.4, 50.1, 66.8, 0]`\n",
    "\n",
    "> **Key Observation:** Neuron 3 (value 30) was active both times, but it was paired with different neighbors. It cannot \"trust\" its neighbors to make a decision.\n",
    "\n",
    "\n",
    "**4. Train vs. Test (The Crucial Mode Switch)**\n",
    "\n",
    "\n",
    "* **Training Phase (`model.train()`):**\n",
    "    * **Dropout is ON.**\n",
    "    * The network is \"intoxicated\" (noisy).\n",
    "    * The random mask is applied, and values are scaled by $\\frac{1}{1-p}$.\n",
    "    * *Objective:* Learn robustness.\n",
    "\n",
    "* **Testing / Inference Phase (`model.eval()`):**\n",
    "    * **Dropout is OFF.**\n",
    "    * We want to use the full \"brain\" capacity for the best prediction.\n",
    "    * All neurons are used: $m = [1, 1, 1, 1, 1]$.\n",
    "    * **No extra scaling** is performed (because we already divided by $(1-p)$ during training, the expected values match).\n",
    "    * *Objective:* Deterministic, maximum performance.\n",
    "\n",
    "\n",
    "**5. When do you apply dropout ?**\n",
    "\n",
    "It is standard practice to apply Dropout **only during the MLP phase**.\n",
    "* **Reason:** Convolutional layers have significantly fewer parameters than Fully Connected (MLP) layers due to weight sharing. Consequently, they are naturally less prone to overfitting, so there is usually no need to aggressively reduce the model's capacity during the feature extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcf09561",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"assets/dropout.gif\" \n",
       "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"assets/dropout.gif\" \n",
    "         style=\"width: 80%; border-radius: 6px; box-shadow: 0 4px 10px rgba(0,0,0,0.15);\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9da28",
   "metadata": {},
   "source": [
    "*Note: This animation was not created by us; it was sourced from the [ManimML GitHub repository](https://github.com/helblazer811/ManimML/blob/main/assets/readme/dropout.gif).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225abbc",
   "metadata": {},
   "source": [
    "## 4. Example of how a CNN learns (The Full Modern Pipeline)\n",
    "\n",
    "This section details the actual learning mechanism of a modern Convolutional Neural Network (CNN), integrating crucial stabilization and regularization techniques (**Data Augmentation, Batch Normalization, Dropout**).\n",
    "\n",
    "**The Setup:**\n",
    "Assume a dataset of **10,000 images**:\n",
    "- **8,000** for training ($N_{train}$).\n",
    "- **2,000** for testing.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Intuition vs. Reality\n",
    "\n",
    "**The Naive Intuition:**\n",
    "One might think the network loops through images one by one:\n",
    "1. Load image 1 $\\to$ Forward $\\to$ Backward $\\to$ Update weights.\n",
    "2. Load image 2 $\\to$ Forward $\\to$ Backward $\\to$ Update weights.\n",
    "*This would be incredibly slow and unstable (noisy gradients).*\n",
    "\n",
    "**The Modern Reality (Vectorization):**\n",
    "We process images in **Mini-Batches**.\n",
    "Let's choose a **Batch Size ($B$) = 32**.\n",
    "\n",
    "The input is not a list of images, but a single **4-Dimensional Tensor**:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{(B,\\; C,\\; H,\\; W)}\n",
    "$$\n",
    "*Example:* 32 RGB images of size $64 \\times 64$ becomes a tensor of shape `(32, 3, 64, 64)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 The Forward Pass (Step-by-Step with Layers)\n",
    "\n",
    "For a single batch of 32 images, the data flows through the pipeline as follows. Note that **Training Mode** behaviors are active here.\n",
    "\n",
    "#### **Step 0: Data Augmentation (CPU/GPU)**\n",
    "Before entering the network (or at the very first layer), we artificially corrupt the data to reduce overfitting.\n",
    "* *Action:* Random flips, rotations, or zooms applied to the batch.\n",
    "* *Result:* The network never sees the exact same image twice.\n",
    "    $$X_{aug} = \\text{Augment}(X)$$\n",
    "\n",
    "#### **Step 1: The Convolutional Block (Feature Extraction)**\n",
    "The tensor $X_{aug}$ enters the layers.\n",
    "1.  **Conv2D:** Extracts features using filters ($W$).\n",
    "    $$Z = X_{aug} * W$$\n",
    "2.  **Batch Normalization (2D):** Stabilizes learning by recentering the data. It uses the batch mean $\\mu_B$ and variance $\\sigma^2_B$.\n",
    "    $$\\hat{Z} = \\frac{Z - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
    "3.  **Activation (ReLU):** Adds non-linearity.\n",
    "    $$A = \\max(0, \\hat{Z})$$\n",
    "4.  **Pooling:** Downsamples dimensions (e.g., MaxPool).\n",
    "\n",
    "#### **Step 2: Flattening**\n",
    "The 4D tensor is flattened into a 2D matrix of shape $(B, \\text{Features})$ to enter the MLP.\n",
    "\n",
    "#### **Step 3: The MLP Block (Classification)**\n",
    "1.  **Dense (Linear):**\n",
    "    $$Z_{dense} = A_{flat} \\cdot W_{dense} + b$$\n",
    "2.  **Batch Normalization (1D):** Normalizes the vector features.\n",
    "3.  **Activation (ReLU):** $\\max(0, Z_{norm})$.\n",
    "4.  **Dropout:** Randomly sets a percentage (e.g., 50%) of neurons to zero to force robustness.\n",
    "    $$A_{final} = A_{relu} \\odot \\text{Mask}_{Bernoulli}$$\n",
    "\n",
    "#### **Step 4: Output & Loss**\n",
    "* **Prediction:** The final layer produces logits, converted to probabilities via **Softmax** (or Sigmoid).\n",
    "* **Loss Computation:** The Loss $\\mathcal{L}$ is calculated as the **average** of the errors of the 32 images.\n",
    "    $$\\mathcal{L}_{batch} = \\frac{1}{B} \\sum_{i=1}^{B} \\text{Loss}(\\text{pred}_i, \\text{target}_i)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Backpropagation & Optimization\n",
    "\n",
    "Once $\\mathcal{L}_{batch}$ is computed:\n",
    "1.  **Backward Pass:** We compute the gradient of the loss w.r.t. **all** parameters $\\theta$ (kernels $\\gamma, \\beta$ of BN, weights, biases).\n",
    "    $$\\nabla_{\\theta} \\mathcal{L}_{batch}$$\n",
    "2.  **Optimizer Step:** We update the weights using Gradient Descent (e.g., Adam/SGD).\n",
    "    $$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}_{batch}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Detailed Iteration Math (Epochs vs. Steps)\n",
    "\n",
    "It is crucial to distinguish between an **Iteration** and an **Epoch**.\n",
    "\n",
    "* **1 Iteration (or Step):** One forward/backward pass of **1 Batch** (32 images).\n",
    "* **1 Epoch:** The network has seen the **entire** dataset (8,000 images) once.\n",
    "\n",
    "**Calculations:**\n",
    "* Dataset Size: $N = 8000$\n",
    "* Batch Size: $B = 32$\n",
    "* **Iterations per Epoch:** $\\frac{8000}{32} = \\mathbf{250}$ steps.\n",
    "\n",
    "If we train for **20 Epochs**:\n",
    "$$\n",
    "\\text{Total Parameter Updates} = 250 \\times 20 = \\mathbf{5,000 \\text{ updates}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 Crucial Note: Train vs. Eval Mode\n",
    "\n",
    "The behavior of these layers changes radically after training:\n",
    "\n",
    "| Layer | **Training Mode** (`model.train()`) | **Evaluation/Test Mode** (`model.eval()`) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data Augmentation** | **ON** (Random variations) | **OFF** (Use original images) |\n",
    "| **Batch Normalization** | Uses **current batch** statistics ($\\mu_B, \\sigma_B$) | Uses **saved running average** statistics |\n",
    "| **Dropout** | **ON** (Randomly kills neurons) | **OFF** (Uses all neurons, no scaling) |\n",
    "\n",
    "### 4.6 Summary\n",
    "\n",
    "Modern CNN training is not a loop over images. It is a sequence of **tensor operations** on batches.\n",
    "1.  **Augment** the batch.\n",
    "2.  Pass through **Conv $\\to$ BN $\\to$ ReLU $\\to$ Pool**.\n",
    "3.  Pass through **Dense $\\to$ BN $\\to$ ReLU $\\to$ Dropout**.\n",
    "4.  Compute **Batch Loss**.\n",
    "5.  Update parameters (1 Step).\n",
    "6.  Repeat 250 times to complete 1 Epoch.\n",
    "\n",
    "\n",
    "### 4.7 Warning !\n",
    "\n",
    "We repeat ourselves here but we do want to insist on the fact the intuitive expectation is that **CNNs process images one by one inside each batch and perform a separate forward for each image is FALSE**\n",
    "\n",
    "In fact:\n",
    "\n",
    "- a batch is represented as a single 4-D tensor,  \n",
    "- one vectorized forward pass processes all images of one batch simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e452a",
   "metadata": {},
   "source": [
    "## 5. Network Depth vs. Information Precision\n",
    "\n",
    "The more convolutional layers we add, the more complex the detected features become.\n",
    "\n",
    "### 5.1 The Mathematical Intuition\n",
    "It is actually quite natural.\n",
    "Mathematically, a neural network is a **composition of non-linear functions**.\n",
    "* One layer performs a simple transformation: $y = f(x)$.\n",
    "* Ten layers perform a chain of transformations: $y = f_9(f_8(...f_1(x)...))$.\n",
    "\n",
    "By stacking layers, we are building a function of **increasing complexity**.\n",
    "Just like in math where composing simple functions allows you to describe complex curves, stacking convolutions allows the network to model extremely precise and intricate relationships in the data.\n",
    "\n",
    "### 5.2 The Hierarchy of Features\n",
    "Because of this composition, the network learns in a hierarchical way. As we move deeper into the network (towards the output), the features become more abstract and \"human-level\":\n",
    "\n",
    "1.  **First Layers:** They detect **simple geometry** (horizontal lines, vertical edges, color gradients).\n",
    "2.  **Middle Layers:** They combine lines to detect **shapes and parts** (corners, curves, circles, textures).\n",
    "3.  **Last Layers:** They combine shapes to recognize **complex objects** (eyes, mouths, ears, car wheels, faces).\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion:** The deeper we go, the more the network understands \"concepts\" (Is there an eye?) rather than just pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979b887",
   "metadata": {},
   "source": [
    "## 6. Bonus: Mathematical Zoom into Backpropagation in CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac454e4",
   "metadata": {},
   "source": [
    "\n",
    "We aimed to keep this document concise while providing a clear overview of the CNN pipeline. As established, a CNN is essentially a large neural network composed of two distinct parts: the **Convolutional block** and the **MLP block**.\n",
    "\n",
    "All parameters, including the values within the kernels, are learned via **Backpropagation**.\n",
    "\n",
    "To understand how a kernel learns, we rely on the **Chain Rule**.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W}}_{\\text{What we want}} ={\\frac{\\partial \\mathcal{L}}{\\partial Y}} \\times {\\frac{\\partial Y}{\\partial W}}\n",
    "$$\n",
    "\n",
    "However, if you try to compute these partial derivatives for a Convolutional Kernel, you will find an **interesting result** due to the architecture's specific nature.\n",
    "\n",
    "---\n",
    "\n",
    "### The Weight Sharing Effect\n",
    "\n",
    "To understand why CNN training is computationally intensive but parameter-efficient, let's derive the gradient for a **single weight** in a concrete example.\n",
    "\n",
    "**The Setup:**\n",
    "* **Input ($X$):** A $3 \\times 3$ matrix (e.g., a small image patch).\n",
    "* **Kernel ($W$):** A $2 \\times 2$ filter.\n",
    "* **Output ($Y$):** A $2 \\times 2$ feature map (valid convolution).\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} x_{11} & x_{12} & x_{13} \\\\ x_{21} & x_{22} & x_{23} \\\\ x_{31} & x_{32} & x_{33} \\end{bmatrix}, \\quad\n",
    "W = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**1. The Forward Pass (Weight Sharing in Action)**\n",
    "Let's calculate the output map $Y$. Notice how the **same weight** $w_{11}$ (top-left of kernel) is reused 4 times, interacting with different input pixels.\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix} y_{11} & y_{12} \\\\ y_{21} & y_{22} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The equations for the pixels are:\n",
    "* $y_{11} = {\\color{red}w_{11}}x_{11} + w_{12}x_{12} + w_{21}x_{21} + w_{22}x_{22}$\n",
    "* $y_{12} = {\\color{red}w_{11}}x_{12} + w_{12}x_{13} + w_{21}x_{22} + w_{22}x_{23}$\n",
    "* $y_{21} = {\\color{red}w_{11}}x_{21} + w_{12}x_{22} + w_{21}x_{31} + w_{22}x_{32}$\n",
    "* $y_{22} = {\\color{red}w_{11}}x_{22} + w_{12}x_{23} + w_{21}x_{32} + w_{22}x_{33}$\n",
    "\n",
    "**2. The Backward Pass (Computing the Gradient)**\n",
    "Assume we receive the gradient of the Loss $\\mathcal{L}$ with respect to the output $Y$ (let's call it $\\delta$, the \"Message from the future\"):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y} = \\delta = \\begin{bmatrix} \\delta_{11} & \\delta_{12} \\\\ \\delta_{21} & \\delta_{22} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to find the gradient for the weight ${\\color{red}w_{11}}$: **$\\frac{\\partial \\mathcal{L}}{\\partial w_{11}}$**.\n",
    "\n",
    "**Applying the Multivariate Chain Rule:**\n",
    "Since $w_{11}$ contributed to **all 4 output pixels**, a change in $w_{11}$ affects the error through 4 different paths. We must sum these contributions.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} =\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{11}}\\frac{\\partial y_{11}}{\\partial w_{11}} +\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{12}}\\frac{\\partial y_{12}}{\\partial w_{11}} +\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{21}}\\frac{\\partial y_{21}}{\\partial w_{11}} +\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{22}}\\frac{\\partial y_{22}}{\\partial w_{11}}\n",
    "$$\n",
    "\n",
    "**3. Computing the Partial Derivatives:**\n",
    "Looking at the equations in Step 1, we can see the derivative of $y$ with respect to $w_{11}$ is simply the input pixel $x$ it was multiplied with:\n",
    "* $\\frac{\\partial y_{11}}{\\partial w_{11}} = x_{11}$\n",
    "* $\\frac{\\partial y_{12}}{\\partial w_{11}} = x_{12}$\n",
    "* $\\frac{\\partial y_{21}}{\\partial w_{11}} = x_{21}$\n",
    "* $\\frac{\\partial y_{22}}{\\partial w_{11}} = x_{22}$\n",
    "\n",
    "**4. The Final Result:**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} = \\delta_{11}x_{11} + \\delta_{12}x_{12} + \\delta_{21}x_{21} + \\delta_{22}x_{22}\n",
    "$$\n",
    "\n",
    "**Conclusion:**\n",
    "The gradient of a single kernel weight is not just a local calculation. It is the **sum over the entire spatial domain** of the input weighted by the backpropagated error.\n",
    "\n",
    "In simpler terms: **$\\nabla W$ corresponds to the convolution of the Input $X$ and the Error $\\delta$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4e7c8",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "\thttps://www.youtube.com/watch?v=JfBf5eYptSs&t=1468s\n",
    "\thttps://www.youtube.com/watch?v=zG_5OtgxfAg&t=636s\n",
    "\thttps://www.youtube.com/watch?v=581X9wsnWJs&t=700s\n",
    "\thttps://www.youtube.com/watch?v=zPwQiZFrwUY\n",
    "\thttps://www.kaggle.com/code/blurredmachine/alexnet-architecture-a-complete-guide\n",
    "\thttps://medium.com/biased-algorithms/batch-normalization-in-cnn-81c0bd832c63\n",
    "\thttps://www.youtube.com/watch?v=h8oL4GXkBV4\n",
    "\thttps://github.com/helblazer811/ManimML/blob/main/assets/readme/dropout.gif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
