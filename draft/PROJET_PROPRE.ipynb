{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bab66a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# CNN : Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3600e9",
   "metadata": {},
   "source": [
    "A project made by : Walid Canesse & Salaheddin Ben Emran & Mohamed-Ayoub Bouzid\n",
    "\n",
    "All animations have been made by ourselves using Manim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d001b67",
   "metadata": {},
   "source": [
    "METTRE UN SOMMAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad61e9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86cc28",
   "metadata": {},
   "source": [
    "In Machine Learning, when we need to perform classification, we have many standard models that work well, such as:\n",
    "* **Logistic Regression**\n",
    "* **Random Forest**\n",
    "* **SVM (Support Vector Machines)**\n",
    "* **K-Nearest Neighbors**\n",
    "\n",
    "However, in this notebook, we will focus on a specific case: **when the input is an image**.\n",
    "\n",
    "We will demonstrate that **Neural Networks** are a much more powerful tool for this task. In particular, we will see how their structure—with a specific adaptation called **Convolution**—can be perfectly tailored to understand and classify visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e9c72",
   "metadata": {},
   "source": [
    "# CHAPTER 1 : The intuition behind CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0a079",
   "metadata": {},
   "source": [
    "## 0. What is an Image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc4fc6",
   "metadata": {},
   "source": [
    "To a computer, an image is just a grid of numbers. However, in Deep Learning, we formalize this using the concept of **Tensors**.\n",
    "\n",
    "Dimension of an image : $Height \\times Width \\times Channels$\n",
    "\n",
    "### A. Grayscale Images (Black & White)\n",
    "We define a grayscale image as a volume of size **$N_1 \\times N_2 \\times 1$** .\n",
    "* It is a matrix where each pixel is a number between **0 and 255**.\n",
    "    * **0:** Pure Black.\n",
    "    * **255:** Pure White.\n",
    "    * **Between:** Shades of gray.\n",
    "* We can view this as a **Tensor with 1 channel** (a single \"slice\" or matrix).\n",
    "\n",
    "### B. Color Images (RGB)\n",
    "A color image is a volume of size **$N_1 \\times N_2 \\times 3$** \n",
    "* Instead of one slice, we have **3 matrices stacked together**:\n",
    "    1.  **Red** Matrix\n",
    "    2.  **Green** Matrix\n",
    "    3.  **Blue** Matrix\n",
    "* This is a **3D Tensor** with 3 channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952aaa0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5fc093d",
   "metadata": {},
   "source": [
    "\n",
    "### C. What is a Tensor?\n",
    "A **Tensor** is simply a generalization of matrices to higher dimensions.\n",
    "\n",
    "* **0D Tensor (Scalar):** A simple number.\n",
    "    * *Structure:* Just a singular point.\n",
    "    * *Example:* `5`\n",
    "\n",
    "* **1D Tensor (Vector):** A simple list of numbers.\n",
    "    * *Structure:* Just a line.\n",
    "    * *Example:* `[0, 255, 12, 45]`\n",
    "\n",
    "* **2D Tensor (Matrix):** A single grid of numbers.\n",
    "    * *Structure:* A \"sheet\" with rows and columns.\n",
    "    * *Example:* A single grayscale image map ($H \\times W \\times 1$).\n",
    "\n",
    "* **3D Tensor:** **Matrices stacked together**.\n",
    "    * *Structure:* A stack of sheets.\n",
    "    * *Example:* A Color Image. It is **3 matrices** (Red, Green, Blue) stacked on top of each other ($H \\times W \\times 3$).\n",
    "\n",
    "### Important Note: \n",
    "**A 3D Tensor is not limited to 3 channels.**\n",
    "\n",
    "* You can stack **2,5, 7, or even 100 matrices** together.\n",
    "* It remains a **3D Tensor** because it is still defined by 3 axes: $Height \\times Width \\times Channels$.\n",
    "* **Think of a book:** Whether a book has 3 pages or 500 pages, it is still a \"3D object\". The number of pages (channels) changes, but the structure is the same.\n",
    "\n",
    "\n",
    "**Intuition:** Don't overcomplicate it. Just see a Tensor as a container of matrices stacked on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33634851",
   "metadata": {},
   "source": [
    "METTRE UNE ANIMATION POUR LES TENSEURS , GENRE FAIRE TENSEUR 1D CEST UN POINT ,2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca59ad",
   "metadata": {},
   "source": [
    "## 1. Why Not Classic MLPs? (Example: MNIST Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c62e8",
   "metadata": {},
   "source": [
    "Let's start with a classic example: the **MNIST dataset**.\n",
    "This is a database of handwritten digits (0 to 9). The images are **Grayscale** and have a size of **$28 \\times 28 \\times 1$**.\n",
    "\n",
    "As we discussed, grayscale images are simply matrices of pixels where values range from **0 to 255**.\n",
    "\n",
    "### The Naive Approach: Multi-Layer Perceptron (MLP)\n",
    "If we want to build a model to classify these digits, the first method that comes to mind is a standard **Fully Connected Neural Network** (or MLP).\n",
    "\n",
    "However, an MLP expects a **flat vector** as input, not a matrix.\n",
    "To use it, we are forced to **flatten** our $28 \\times 28$ image into a single vector of **784 pixels**. We then feed this long vector into the MLP to get a prediction.\n",
    "\n",
    "\n",
    "\n",
    "[Image of flattening image to vector]\n",
    "\n",
    "\n",
    "### Why is this method not optimal?\n",
    "While this might work for very simple tasks, it has **three major problems**:\n",
    "\n",
    "#### 1. Loss of Spatial Information\n",
    "An image is not just a random list of numbers; it is a **2D structured object**.\n",
    "* Neighboring pixels are correlated (they work together to form shapes).\n",
    "* The order and position matter.\n",
    "**Problem:** By flattening the image into a vector, we **break this structure**. The network reads the image \"pixel by pixel\" without understanding the geometry, leading to a poor use of visual information.\n",
    "\n",
    "#### 2. Giant Number of Parameters\n",
    "Let's do the math for a simple MLP with **2 hidden layers of 100 neurons each** and **10 output neurons**.\n",
    "\n",
    "* **Input Size:** 784 (pixels)\n",
    "* **Hidden Layer 1:** 100 neurons\n",
    "* **Hidden Layer 2:** 100 neurons\n",
    "* **Output Layer:** 10 neurons\n",
    "\n",
    "**Detailed Calculation of Parameters (Weights + Biases):**\n",
    "\n",
    "* **Layer 1 (Input $\\to$ Hidden 1):**\n",
    "    * Weights: $784 \\times 100 = 78,400$\n",
    "    * Biases: $100$ (1 per neuron)\n",
    "    \n",
    "* **Layer 2 (Hidden 1 $\\to$ Hidden 2):**\n",
    "    * Weights: $100 \\times 100 = 10,000$\n",
    "    * Biases: $100$\n",
    "\n",
    "* **Layer 3 (Hidden 2 $\\to$ Output):**\n",
    "    * Weights: $100 \\times 10 = 1,000$\n",
    "    * Biases: $10$\n",
    "\n",
    "**Total Sum:**\n",
    "$$78,400 + 100 + 10,000 + 100 + 1,000 + 10 = \\mathbf{89,610}$$\n",
    "\n",
    "**Problem:** We have nearly **90,000 parameters** just for a tiny $28 \\times 28$ black and white image. On realistic color images (e.g., $1000 \\times 1000$), this number would explode into the billions, making the model impossible to train efficiently.\n",
    "\n",
    "#### 3. No Translation Invariance\n",
    "To a human, if a digit \"5\" moves a little bit to the left or right, it is clearly still a \"5\". We recognize the **shape**, no matter where it is.\n",
    "**Problem:** An MLP looks at each specific pixel position separately.\n",
    "* If the \"5\" shifts, different input pixels light up.\n",
    "* To the MLP, this looks like a completely different input.\n",
    "* It has to \"re-learn\" what a \"5\" looks like for every possible position in the image. It lacks **Translation Invariance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c04a1",
   "metadata": {},
   "source": [
    "## 2. How Humans Classify Images: Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0780a",
   "metadata": {},
   "source": [
    "To understand the intuition behind CNNs, let's look at how humans see.\n",
    "\n",
    "When we look at an image, **we do not scan every single pixel one by one.**\n",
    "Instead, we process the image globally. We unconsciously look for **Features**:\n",
    "* **Edges** (contours)\n",
    "* **Textures**\n",
    "* **Patterns** (e.g., the shape of an ear, the curve of a digit).\n",
    "\n",
    "**The Decision Process:**\n",
    "Identifying these features increases the probability of a specific class. If we see \"whiskers\" + \"pointed ears\" + \"fur\", our brain concludes: *\"Yes, this looks like a cat.\"*\n",
    "\n",
    "**The Goal of CNNs:**\n",
    "Convolutional Neural Networks are designed to **mimic this exact behavior**.\n",
    "The key is the convolution operator who acts like an **Automatic Feature Detector**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582d743",
   "metadata": {},
   "source": [
    "# CHAPTER 2 : The convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2007a",
   "metadata": {},
   "source": [
    "## 4. Defining Image Convolution and Kernels\n",
    "\n",
    "As we said, the convolution acts as a **Feature Detector**. But how exactly do we compute it?\n",
    "\n",
    "**The Process:**\n",
    "1.  **Input:** We take an image (pixel matrix).\n",
    "2.  **Kernel:** We define a small filter of a specific size (e.g., $3 \\times 3$) that we choose.\n",
    "3.  **Operation:** We slide the kernel over the image, performing a \"dot product\" (multiplication + sum) at every position.\n",
    "\n",
    "### Let's Calculate!\n",
    "FAIRE 4 EXEMPLES VOIR REMARKABLE \n",
    "ESSAYER DE LES ANIMER !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fd415",
   "metadata": {},
   "source": [
    "## 5. Output Dimensions, Tensors, and Calculation Cases\n",
    "\n",
    "ANIMATIONS !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5a01d",
   "metadata": {},
   "source": [
    "## 6. Hyperparameters of Convolution\n",
    "\n",
    "When designing a CNN, we don't just \"apply convolution.\" We have to tune specific knobs—called **Hyperparameters**—to control how the network processes the image.\n",
    "\n",
    "### 1. Padding\n",
    "**The Problem:** Without padding, the pixels on the borders are \"seen\" less often by the filters because we cannot center the kernel on them. (Try a simple example yourself: you will notice that edge pixels are involved in far fewer calculations than the central pixels).\n",
    "\n",
    "**The Solution:** Padding consists of adding a border of pixels (usually **Zeros**) around the input image.\n",
    "* **Benefit:** It allows us to process the edges as effectively as the center.\n",
    "\n",
    "\n",
    "EXAMPLE ANIMATION\n",
    "\n",
    "### 2. Stride\n",
    "**The Concept:** The Stride is the \"step size\" of the convolution.\n",
    "* **Stride = 1:** We shift the filter **1 pixel** at a time. This is the standard detailed scan.\n",
    "* **Stride = 2:** We shift the filter **2 pixels** at a time (we skip one pixel).\n",
    "**Impact:** A larger stride **reduces the output size**  and speeds up computation because we perform fewer operations.\n",
    "\n",
    "EXAMPLE ANIMATION\n",
    "\n",
    "### 3. The Output Dimension Formula \n",
    "\n",
    "To make the math easier, let's assume we are working with square inputs and square filters (which is almost always the case).\n",
    "* **$N$**: Input dimension (Height = Width).\n",
    "* **$F$**: Filter dimension (Height = Width).\n",
    "* **$P$**: Padding.\n",
    "* **$S$**: Stride.\n",
    "\n",
    "The formula to calculate the output size of the feature map is:\n",
    "\n",
    "$$\n",
    "\\text{Output Size} = \\left\\lfloor \\frac{N - F + 2P}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "> **Example:**\n",
    "> * Input Image ($N$): $28$ (for a $28 \\times 28$ image)\n",
    "> * Filter ($F$): $3$ (for a $3 \\times 3$ kernel)\n",
    "> * Padding ($P$): $1$\n",
    "> * Stride ($S$): $1$\n",
    ">\n",
    "> $$\\text{Size} = \\frac{28 - 3 + (2 \\times 1)}{1} + 1 = \\frac{27}{1} + 1 = \\mathbf{28}$$\n",
    "> *Result:* We kept the same size ($28 \\times 28$) thanks to the padding!\n",
    "\n",
    "### 4. Kernel Size & Number of Filters\n",
    "Finally, we must choose the properties of the filters themselves:\n",
    "\n",
    "* **Kernel Size (e.g., $3 \\times 3$ or $5 \\times 5$):**\n",
    "    * This controls the **Receptive Field** (the local area) the network looks at.\n",
    "    * Small kernels look for fine details. Large kernels look for broader patterns.\n",
    "\n",
    "* **Number of Filters (Depth):**\n",
    "    * This controls **how many features** we want to learn in this layer.\n",
    "    * **More filters** = The network can learn more diverse patterns (edges, textures, colors).\n",
    "    * **Trade-off:** More filters mean more parameters to learn and slower training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b487b",
   "metadata": {},
   "source": [
    "# TRANSITION : ajouter l'idée suivante : on va vouloir faire des convolution de convolution ( plusieurs compositions de convolutions) pour pouvoir ensuite applatir le dernier output et faire un MLP dessus. C'est l'idée principal à retenir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c04abd",
   "metadata": {},
   "source": [
    "# CHAPTER 3 : Building the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cee112",
   "metadata": {},
   "source": [
    "## 7. The CNN Breakthrough: From Manual Filters to Learned Features\n",
    "\n",
    "### The Hypothesis\n",
    "We might ask:\n",
    "> *\"Why don't we just manually choose the best kernels (like standard edge detectors), apply them to the image to extract features, and then feed the result into a classic MLP for classification?\"*\n",
    "\n",
    "### The Problem\n",
    "This approach is flawed for three main reasons:\n",
    "1.  **Missing Hidden Features:** We don't always know intuitively which patterns are the most \"discriminant\" (useful) to distinguish between classes.\n",
    "2.  **Unknown Combinations:** We don't know the optimal mix of kernels to use.\n",
    "3.  **Human Limitation:** By choosing manually, we limit the network to features *we* have imagined.\n",
    "\n",
    "### The Solution: End-to-End Learning\n",
    "The core idea of CNNs is to **let the network learn the values inside the kernels itself.**\n",
    "\n",
    "* **Learnable Parameters:** The coefficients inside the filters are not fixed constants. They are **parameters** (weights) of the network.\n",
    "* **Backpropagation:** Just like in an MLP, these weights are updated during training. The network figures out *on its own* which kernels are best to minimize the error.\n",
    "\n",
    "### The Result: A Giant Pipeline\n",
    "We obtain a single, unified network where:\n",
    "1.  **Convolution Layers (The \"Eyes\"):** Learn to be the best possible **Feature Extractors**.\n",
    "2.  **Fully Connected Layers (The \"Brain\"):** Take these features and handle the **Classification Decision**.\n",
    "\n",
    "\n",
    "\n",
    "[Image of cnn architecture diagram]\n",
    "CNN PIPELINE EN ANIMATION \n",
    "AUSSI EXPLIQUER POURQUOI LA PARTIE CONVOLUTION CA PEUT ETRE VU COMME UNE PARTIE RESEAUX DE NEURONNE !!!!!!!!!! VIA UN EXEMPLE ET DIRE QUE CEST BIEN UNE PARTIE PAS FULLY CONNECTED !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261414ac",
   "metadata": {},
   "source": [
    "## 10. Convolution as a Layer of a Neural Network & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee5a41",
   "metadata": {},
   "source": [
    "## 8. Pooling\n",
    "\n",
    "After convolution, we often add a **Pooling Layer**.\n",
    "Its goal is to reduce the spatial dimensions of the image (Height and Width), which drastically reduces the number of parameters and computation cost.\n",
    "\n",
    "### A. How it Works\n",
    "We define a window of size **$K \\times K$** (just like a kernel) and a **Stride**.\n",
    "\n",
    "* **General Rule:** The window slides over the image. For each window, we compress the pixels into a single value (we calculate either the **max** or the **average** of those values).\n",
    "* **Standard Practice:** In 99% of cases, we use a window of **$2 \\times 2$** with a **Stride of 2**. This effectively divides the height and width of the image by 2 at each step.\n",
    "\n",
    "\n",
    "\n",
    "[Image of max pooling vs average pooling]\n",
    "\n",
    "\n",
    "### B. Max Pooling vs. Average Pooling\n",
    "There are two main ways to compress the data:\n",
    "1.  **Average Pooling:** Calculates the average value of the pixels in the window.\n",
    "2.  **Max Pooling:** Selects only the **maximum value** in the window.\n",
    "\n",
    "### C. Why is Max Pooling Better for Classification?\n",
    "Let's take a concrete example with a $2 \\times 2$ window to understand why Max Pooling is the standard.\n",
    "Imagine a region of the image after a convolution (a feature map):\n",
    "\n",
    "$$\n",
    "\\text{Window} = \\begin{bmatrix} \\mathbf{164} & 0 \\\\ 1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Context:**\n",
    "    * **164:** A very strong activation (the filter detected a sharp feature, like an edge or a texture).\n",
    "    * **0 and 1:** Background noise. Note that in image processing, dark pixels (values near 0) usually do not contain key information; they represent empty space or background.\n",
    "\n",
    "**The Comparison:**\n",
    "\n",
    "* **Average Pooling:**\n",
    "    $$\\frac{164 + 0 + 1 + 0}{4} \\approx \\mathbf{41}$$\n",
    "    * *Result:* The strong signal (164) is **diluted** by the zeros. The information becomes blurry and less distinct.\n",
    "\n",
    "* **Max Pooling:**\n",
    "    $$\\max(164, 0, 1, 0) = \\mathbf{164}$$\n",
    "    * *Result:* We keep the **164**. The pooling ignores the noise (the zeros) and preserves the most important feature.\n",
    "\n",
    "**Conclusion:** For image classification, Max Pooling acts as a \"Feature Selector,\" ensuring the strongest patterns survive, while Average Pooling tends to wash them out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb20a11",
   "metadata": {},
   "source": [
    "## 11. Designing the Pipeline: The Hyperparameters\n",
    "\n",
    "We can divide them into three main categories:\n",
    "\n",
    "### A. Convolution Layers\n",
    "\n",
    "**Step 1: Define Architecture Depth**\n",
    "* **Number of Blocks:** How many (Conv + Pooling) blocks to stack?\n",
    "\n",
    "**Step 2: For EACH Block, choose:**\n",
    "* **Number of Filters:** (e.g., 32, 64, 128...)\n",
    "* **Filter Size (Kernel):** (e.g., $3 \\times 3$)\n",
    "* **Padding:** \n",
    "* **Stride:** \n",
    "* **Activation Function:** (e.g., ReLU,sigmoid)\n",
    "* **Pooling Type:** (Max Pooling vs Average Pooling)\n",
    "\n",
    "### B. MLP Layers (Classifier)\n",
    "* **Number of Layers**\n",
    "* **Neurons per Layer**\n",
    "* **Activation Function:** (e.g., ReLU for hidden, Sigmoid/Softmax for output)\n",
    "\n",
    "### C. Training (Backward)\n",
    "* **Optimizer:** (e.g., Adam, SGD)\n",
    "* **Learning Rate**\n",
    "* **Batch Size**\n",
    "* **Number of Epochs**\n",
    "\n",
    "\n",
    "### D. Crucial Extras: Stabilization & Regularization\n",
    "To make the model robust, we add two specific layers.\n",
    "\n",
    "#### 1. Batch Normalization (Batch Norm)\n",
    "* **Where to put it?** Inside the Conv Block, **between** the Convolution and the Activation function.\n",
    "    * *Order:* `Conv2d` $\\to$ `BatchNorm2d` $\\to$ `ReLU`.\n",
    "* **The Math:** It normalizes the output of the convolution by subtracting the batch mean ($\\mu$) and dividing by the standard deviation ($\\sigma$).\n",
    "    $$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
    "    *(Where $\\gamma$ and $\\beta$ are learnable parameters).*\n",
    "* **Why?** It stabilizes the learning process (keeps values centered), allowing the use of higher learning rates and faster convergence.\n",
    "\n",
    "https://www.youtube.com/watch?v=nT9nKBCjS_Y\n",
    "\n",
    "#### 2. Dropout\n",
    "* **Where to put it?** Inside the MLP, **immediately after** the Activation function of hidden layers.\n",
    "    * *Order:* `Linear` $\\to$ `ReLU` $\\to$ `Dropout`.\n",
    "* **The Math:** During training, for each neuron, we flip a coin with probability $p$.\n",
    "    * If heads: The neuron's output becomes **0** (it is \"turned off\").\n",
    "    * If tails: The neuron works normally.\n",
    "* **Why?** It prevents **Overfitting**. By randomly silencing neurons, the network cannot rely on a single specific path. It forces the \"team\" of neurons to be robust, ensuring that the model generalizes well to new images instead of memorizing the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699eeb6",
   "metadata": {},
   "source": [
    "## 13. Understanding the Training Flow: Batches, Tensors, and Iterations\n",
    "\n",
    "To understand how training really works, we need to look under the hood at the dimensions and the timeline.\n",
    "\n",
    "### A. The Setup: Data and Batches\n",
    "Let's use our concrete example:\n",
    "* **Dataset:** 8,000 Images.\n",
    "* **Image Size:** $28 \\times 28 \\times 1$ (Grayscale).\n",
    "* **Batch Size:** We choose **32**.\n",
    "\n",
    "**Why Batches?**\n",
    "We cannot feed 8,000 images at once into the GPU (memory explosion). We also don't want to feed them 1 by 1 (too slow). We group them into packets of 32.\n",
    "\n",
    "**Key Definitions:**\n",
    "1.  **Iteration (Step):** The act of processing **one batch** (32 images) -> calculating error -> updating weights.\n",
    "2.  **Epoch:** The act of processing the **entire dataset** once.\n",
    "\n",
    "**The Math:**\n",
    "$$\\text{Iterations per Epoch} = \\frac{8000 \\text{ images}}{32 \\text{ batch size}} = \\mathbf{250 \\text{ Iterations}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### B. Step-by-Step: Inside a Single Iteration\n",
    "Let's zoom in on **Iteration #1** (the very first step of training).\n",
    "\n",
    "#### 1. The Input Tensor (4D)\n",
    "The computer does not load images one by one. It creates a massive 4-Dimensional block containing the first 32 images stacked together.\n",
    "\n",
    "* **Format:** $(Batch, Channels, Height, Width)$\n",
    "* **Dimensions:** $(32, 1, 28, 28)$\n",
    "\n",
    "#### 2. The Convolution Layer (Parallel Processing)\n",
    "We defined a Convolution layer with:\n",
    "* **32 Filters** (Kernel size $3 \\times 3$).\n",
    "* **Stride 1**, **No Padding**.\n",
    "\n",
    "**The Operation:**\n",
    "The GPU applies the 32 filters to the 32 images **simultaneously**. It does not use a \"for loop\". It performs a massive matrix calculation on the whole block.\n",
    "\n",
    "**Calculating Output Size:**\n",
    "* Spatial size: $28 - 3 + 1 = 26$ (Output is $26 \\times 26$).\n",
    "* Depth: We have 32 Filters, so output depth is 32.\n",
    "* Batch: We still have 32 images.\n",
    "\n",
    "**The Output Tensor Dimensions:**\n",
    "$$(32, 32, 26, 26)$$\n",
    "*(Batch Size $\\times$ Number of Filters $\\times$ Height $\\times$ Width)*\n",
    "\n",
    "#### 3. The Batch Normalization (The Deep Dive)\n",
    "Now, this huge tensor $(32, 32, 26, 26)$ enters the Batch Norm layer.\n",
    "The goal is to calculate the **Mean ($\\mu$)** and **Standard Deviation ($\\sigma$)** to clean the data.\n",
    "\n",
    "**How is the mean calculated?**\n",
    "Batch Norm works **Filter by Filter** (Channel by Channel). It does NOT mix the filters.\n",
    "\n",
    "**Example: Processing Filter #1**\n",
    "Batch Norm looks at the specific \"slice\" for Filter #1 across the ENTIRE batch.\n",
    "* It takes the $26 \\times 26$ result of Filter 1 for **Image 1**.\n",
    "* It takes the $26 \\times 26$ result of Filter 1 for **Image 2**.\n",
    "* ...\n",
    "* It takes the $26 \\times 26$ result of Filter 1 for **Image 32**.\n",
    "\n",
    "It collects all these pixels together:\n",
    "$$\\text{Total Pixels} = 32 \\times 26 \\times 26 = \\mathbf{21,632 \\text{ pixels}}$$\n",
    "\n",
    "It calculates **ONE single average** and **ONE standard deviation** from these 21,632 values.\n",
    "Then, it subtracts this average from every pixel in Filter #1 (across all 32 images).\n",
    "\n",
    "*It repeats this process for Filter #2, Filter #3... up to Filter #32.*\n",
    "\n",
    "---\n",
    "\n",
    "### C. The Timeline: The Training Loop\n",
    "Now that we understand what happens inside one step, here is the full timeline of training.\n",
    "\n",
    "**Epoch 1 Starts:**\n",
    "\n",
    "1.  **Time T=0 (Iteration 1):**\n",
    "    * Load **Batch 1** (Images 0 to 31).\n",
    "    * **Forward Pass:** Calculate Conv -> BatchNorm -> ReLU -> MLP -> Final Prediction.\n",
    "    * **Loss Calculation:** Compare predictions of these 32 images to the real labels.\n",
    "    * **Backward Pass:** Update the weights of the network immediately.\n",
    "\n",
    "2.  **Time T=1 (Iteration 2):**\n",
    "    * The network is now slightly \"smarter\" (weights have changed).\n",
    "    * Load **Batch 2** (Images 32 to 63).\n",
    "    * Forward -> Loss -> Backward -> Update.\n",
    "\n",
    "3.  **... (Repeat 248 times) ...**\n",
    "\n",
    "4.  **Time T=250 (Iteration 250):**\n",
    "    * Load the final Batch.\n",
    "    * Update weights.\n",
    "\n",
    "**End of Epoch 1:** The network has seen every image exactly once.\n",
    "**Start Epoch 2:** We shuffle the images and start again from Batch 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e452a",
   "metadata": {},
   "source": [
    "## 12. Network Depth vs. Information Precision\n",
    "\n",
    "The more convolutional layers we add, the more complex the detected features become.\n",
    "\n",
    "### A. The Mathematical Intuition\n",
    "It is actually quite natural.\n",
    "Mathematically, a neural network is a **composition of non-linear functions**.\n",
    "* One layer performs a simple transformation: $y = f(x)$.\n",
    "* Ten layers perform a chain of transformations: $y = f_9(f_8(...f_1(x)...))$.\n",
    "\n",
    "By stacking layers, we are building a function of **increasing complexity**.\n",
    "Just like in math where composing simple functions allows you to describe complex curves, stacking convolutions allows the network to model extremely precise and intricate relationships in the data.\n",
    "\n",
    "### B. The Hierarchy of Features\n",
    "Because of this composition, the network learns in a hierarchical way. As we move deeper into the network (towards the output), the features become more abstract and \"human-level\":\n",
    "\n",
    "1.  **First Layers:** They detect **simple geometry** (horizontal lines, vertical edges, color gradients).\n",
    "2.  **Middle Layers:** They combine lines to detect **shapes and parts** (corners, curves, circles, textures).\n",
    "3.  **Last Layers:** They combine shapes to recognize **complex objects** (eyes, mouths, ears, car wheels, faces).\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion:** The deeper we go, the more the network understands \"concepts\" (Is there an eye?) rather than just pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e0780",
   "metadata": {},
   "source": [
    "# CHAPTER 4: Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f501d7",
   "metadata": {},
   "source": [
    "## 13. Case Study: Breast Cancer Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b3702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
